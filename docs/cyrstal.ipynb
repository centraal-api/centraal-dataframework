{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysftp==0.2.9\n",
    "#Imports presentacion\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports generales\n",
    "import pysftp\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar directorio CDF y Configuraciones\n",
    "sys.path.append('../')\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Importar CDF\n",
    "from centraal_dataframework.resources import datalake\n",
    "from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "from centraal_dataframework.tasks import task_dq, task\n",
    "from centraal_dataframework.excepciones import ErrorTareaCalidadDatos\n",
    "from centraal_dataframework.resources import GreatExpectationsToolKit\n",
    "from centraal_dataframework.runner import Runner\n",
    "\n",
    "# Preparación de ambiente\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENEDOR_DESTINO = os.environ.get('datalake_cleandir', 'cleansed-zone')\n",
    "WD = os.environ.get('datalake_workdir', 'raw-zone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# registro de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea get_scrapping_file sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "# Traer Archivos SFTP\n",
    "@task\n",
    "def get_scrapping_file(datalake, logger):\n",
    "    # Variables\n",
    "    last_file_date = 0\n",
    "    last_file_name = ''\n",
    "    scrapping_file = None\n",
    "    \n",
    "    # Abrir conexión\n",
    "    cnopts = pysftp.CnOpts()\n",
    "    cnopts.hostkeys = None\n",
    "    sftp = pysftp.Connection(\n",
    "        host=os.environ['sftp_servidor'],\n",
    "        port=int(os.environ['sftp_port']),\n",
    "        username=os.environ['sftp_usuario'],\n",
    "        password=os.environ['sftp_clave'],\n",
    "        cnopts=cnopts,\n",
    "    )\n",
    "    sftp.cwd(os.environ['sftp_raiz'])\n",
    "    # Buscar el último archivo\n",
    "    archivos = sftp.listdir_attr()\n",
    "    for archivo in archivos:\n",
    "        if archivo.longname[0] != 'd':\n",
    "            if archivo.st_atime > last_file_date:\n",
    "                last_file_date = archivo.st_atime\n",
    "                last_file_name = archivo.filename\n",
    "    # Cargar el archivo al DataFrame\n",
    "    with sftp.open(last_file_name) as sfile:\n",
    "        scrapping_file = pd.read_csv(sfile, sep=',', encoding='latin1')\n",
    "        sftp.close()\n",
    "    # Escribimos el DataFrame en nuestro raw-zone\n",
    "    csv_output_dir = f\"{CONTENEDOR}/{WD}\"\n",
    "    datalake.write_csv(scrapping_file, f\"{csv_output_dir}/scrapping.csv\", sep='|', index=False, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea scrapping_validate_prerequisites sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def scrapping_validate_prerequisites(datalake, gx_toolkit: GreatExpectationsToolKit, logger):\n",
    "    \"\"\"Valida los pre-requisitos básicos del archivo de Scrapping\"\"\"\n",
    "    csv_input_dir = WD + '/scrapping/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"scrapping.csv\", sep=\"|\", encoding='latin1')\n",
    "    logger.info(\"Validando prerrequisitos del archivo Scrapping...\")\n",
    "\n",
    "    # Nombres de columnas\n",
    "    scrapping_column_names = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_columns_to_match_set\",\n",
    "        kwargs={\n",
    "            \"column_set\": [\n",
    "                \"date\",\n",
    "                \"canal\",\n",
    "                \"category\",\n",
    "                \"subcategory\",\n",
    "                \"subcategory2\",\n",
    "                \"subcategory3\",\n",
    "                \"marca\",\n",
    "                \"modelo\",\n",
    "                \"sku\",\n",
    "                \"upc\",\n",
    "                \"item\",\n",
    "                \"item characteristics\",\n",
    "                \"url sku\",\n",
    "                \"image\",\n",
    "                \"price\",\n",
    "                \"sale price\",\n",
    "                \"shipment cost\",\n",
    "                \"sales flag\",\n",
    "                \"store id\",\n",
    "                \"store name\",\n",
    "                \"store address\",\n",
    "                \"stock\",\n",
    "                \"upc wm\",\n",
    "                \"final price\",\n",
    "                \"upc wm2\",\n",
    "                \"comp\",\n",
    "                \"composition\",\n",
    "                \"homogenized_clothing\",\n",
    "                \"homogenized_subcategory\",\n",
    "                \"homogenized_category\",\n",
    "                \"homogenized_color\",\n",
    "                \"made_in\",\n",
    "            ],\n",
    "            \"exact_match\": True,\n",
    "            \"result_format\": \"SUMMARY\",\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Las columnas del archivo de Scrapping no concuerdan con las esperadas\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Price Not Null\n",
    "    scrapping_price_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"price\", \"result_format\": \"SUMMARY\"},\n",
    "        meta={\n",
    "            \"notes\": {\"format\": \"markdown\", \"content\": \"Algunos precios del archivo de Scrapping no están presentes\"}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # EXECUTE EXPECTATIONS\n",
    "    url, result = gx_toolkit.run_expectations_on_df(\n",
    "        source, \"SCRAPPING_MANDATORY\", [scrapping_column_names, scrapping_price_notnull]\n",
    "    )\n",
    "    clear_output(wait=True)\n",
    "    if result.success:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "        logger.info(result['url'])\n",
    "        csv_output_dir = f\"{CONTENEDOR_DESITNO}/{WD}/scrapping.csv\"\n",
    "        datalake.write_csv(source, csv_output_dir, sep=\"|\", encoding='latin1')\n",
    "    else:\n",
    "        logger.info('ERROR: Validación fallida, se detiene el proceso.')\n",
    "        logger.info(result['url'])\n",
    "        raise ErrorTareaCalidadDatos(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea scrapping_validate_column_contents sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def scrapping_validate_column_contents(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Identifica inconsistencias en el contenido de las columnas del archivo Scrapping\"\"\"\n",
    "    csv_input_dir = WD + '/' + scrapping + '/'\n",
    "    source = datalake.read_csv(csv_input_dir+\"scrapping.csv\", sep=\"|\", encoding = 'latin1')\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando contenido de columnas del archivo Scrapping...\")\n",
    "    # creaciones de expectativas\n",
    "\n",
    "    scrapping_clothing_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"clothing\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'clothing' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    #homogenized_category NOT NULL\n",
    "    scrapping_homogenized_category_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"homogenized_category\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_category' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #homogenized_subcategory NOT NULL\n",
    "    scrapping_homogenized_subcategory_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"homogenized_subcategory\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_subcategory' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    #homogenized_color NOT NULL\n",
    "    scrapping_homogenized_color_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"homogenized_color\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_color' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    #marca 9 values (up to 11) <- PARAMETER\n",
    "    scrapping_marca_unique = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_unique_value_count_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"marca\",\n",
    "            \"min_value\":     9,\n",
    "            \"max_value\":     11,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_color' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"SCRAPPING_CONSISTENCE\", [scrapping_clothing_notnull, scrapping_homogenized_category_notnull,\n",
    "                                                                              scrapping_homogenized_subcategory_notnull, scrapping_homogenized_color_notnull,\n",
    "                                                                              scrapping_marca_unique])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + scrapping + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'scrapping.csv', sep=\"|\", encoding = 'latin1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea scrapping_validate_prices sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def scrapping_validate_prices(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida el rango de precios del archivo de Scrapping\n",
    "       Según el lote Zara y de las demás marcas\"\"\"\n",
    "    csv_input_dir = WD + '/' + scrapping + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"scrapping.csv\", sep=\"|\", encoding = 'latin1')\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando contenido de columnas del archivo Scrapping...\")\n",
    "    # creaciones de expectativas\n",
    "    #Final Price\n",
    "    #Precio Final Between\n",
    "    ##Zara 12.000 - 4.000.000\n",
    "    scrapping_zara_price_range = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"final price\",\n",
    "            \"min_value\":     12000,\n",
    "            \"max_value\":     4000000,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Para la marca Zara, algunos precios no se encuentran en el rango esperado.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ##Otras - 12.000 - 1.300.000\n",
    "    scrapping_otras_price_range = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"final price\",\n",
    "            \"min_value\":     12000,\n",
    "            \"max_value\":     1300000,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos precios no se encuentran en el rango esperado.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    result_zara  = gx_toolkit.run_expectations_on_df(source[source['canal'] == 'Zara Colombia'], \"SCRAPPING_PRECIOS_ZARA\", [scrapping_zara_price_range])\n",
    "    result_otras = gx_toolkit.run_expectations_on_df(source[source['canal'] != 'Zara Colombia'], \"SCRAPPING_PRECIOS\", [scrapping_otras_price_range])\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    if result_zara['status'] and result_otras['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info('ZARA:  '+result_zara['url'])\n",
    "        logger.info('OTRAS: '+result_otras['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + scrapping + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'scrapping.csv', sep=\"|\", encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea marcaspropias_validate_column_contents sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def marcaspropias_validate_column_contents(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Validar el contenido de las columnas de marcas propias\"\"\"\n",
    "    csv_input_dir = WD + '/' + dataservices + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"marcas_propias.csv\", sep=\",\", encoding='utf8')\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando contenido de columnas del archivo Marcas Propias...\")\n",
    "    # creaciones de expectativas\n",
    "    marcprop_categoria_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"categoria\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'categoria' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Uso NOT NULL\n",
    "    marcprop_use_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"use\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'use' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Tipo Prenda NOT NULL\n",
    "    marcprop_tipo_prenda_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"prendasGenerales\", #??????\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'prendasGenerales' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"MARCAS_PROPIAS_CONSISTENCY\", [marcprop_categoria_notnull, marcprop_use_notnull,\n",
    "                                                                                   marcprop_tipo_prenda_notnull])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + dataservices + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'marcas_propias.csv', sep=\"|\", encoding='utf8')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea homologaciones_validar_cantidad sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def homologaciones_validar_cantidad(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida la cantidad de marcas que existen en el archivo de homologaciones\"\"\"\n",
    "    csv_input_dir = WD + '/' + homologaciones + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"homologaciones.csv\", sep=\",\")\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando cantidad de registros en homologaciones..\")\n",
    "    homologacion_marca_join = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_unique_value_count_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"Marca\",\n",
    "            \"min_value\":     9,\n",
    "            \"max_value\":     11,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'marca' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"CANTIDAD_MARCAS_HOMOLOGACION\", [homologacion_marca_join])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + homologaciones + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'homologaciones.csv', sep=\"|\", encoding='utf8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea ordentallas_validar_join sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def ordentallas_validar_join(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Validación de Orden Tallas para Join\"\"\"\n",
    "    csv_input_dir = WD + '/' + homologaciones + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"orden_tallas.csv\", sep=\",\")\n",
    "    csv_input_dir = WD + '/' + scrapping + '/'\n",
    "    validation_set = datalake.read_csv(csv_input_dir + \"scrapping.csv\", sep=\"|\", encoding='latin1')\n",
    "    tallas_set = validation_set.stock.unique().tolist()\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando JOIN Orden Tallas...\")\n",
    "\n",
    "    ordentallas_marca_join = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_set\",\n",
    "        kwargs={\n",
    "            \"column\":        \"Talla\",\n",
    "            \"value_set\":      tallas_set,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'marca' no están presentes en el archivo de Orden Tallas\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"ORDENTALLA_MARCAS_JOIN\", [ordentallas_marca_join])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + homologaciones + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'orden_tallas.csv', sep=\"|\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea ean_validar_cantidad_registros sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def ean_validar_cantidad_registros(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida la canitdad de EANs\"\"\"\n",
    "    csv_input_dir = WD + '/' + homologaciones + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"ean.csv\", sep=\"|\")\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando Cantidad EAN...\")\n",
    "    ean_unique = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_unique_value_count_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"EAN\",\n",
    "            \"min_value\":     3000,\n",
    "            \"max_value\":     4000,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"La cantidad de productos presentes excede la esperada\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"CANTIDAD_EAN\", [ean_unique])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + homologaciones + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'ean.csv', sep=\"|\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea atributos_validar_referencias sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def atributos_validar_referencias(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida la existencia de las referencias para los atributos\"\"\"\n",
    "    csv_input_dir = WD + '/' + homologaciones + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"atributos.csv\", sep=\"|\")\n",
    "    validation_set = datalake.read_csv(csv_input_dir + \"ean.csv\", sep=\"|\")\n",
    "    referencia_set = validation_set.REFERENCIA.unique().tolist()\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando Cantidad EAN...\")\n",
    "    ean_unique = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_set\",\n",
    "        kwargs={\n",
    "            \"column\":        \"REFERENCIA\",\n",
    "            \"value_set\":      referencia_set,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunas referencias del archivo de Atributos no están definidas en Marcas Propias\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"ATRIBUTOS_REFERENCIAS\", [ean_unique])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + homologaciones + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'atributos.csv', sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Tarea tallasagotadas_validar_join sobrescrita. Si quiere cambiar este comportamiento\n",
      "use la variable de ambiente `SOBREESCRIBIR_TAREA` (valor actual = True)\n"
     ]
    }
   ],
   "source": [
    "@task_dq\n",
    "def tallasagotadas_validar_join(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Veriifca las referencias de las tallas agotadas\"\"\"\n",
    "    csv_input_dir = WD + '/' + homologaciones + '/'\n",
    "    source = datalake.read_csv(csv_input_dir+'tallas_agotadas.csv', sep='|')\n",
    "    validation_set = datalake.read_csv(csv_input_dir+'ean.csv', sep='|')\n",
    "    ean_set = validation_set.EAN.unique().tolist()\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando JOIN Orden Tallas...\")\n",
    "\n",
    "    tallasagotadas_marca_join = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_set\",\n",
    "        kwargs={\n",
    "            \"column\":        \"PARTNUMBER\",\n",
    "            \"value_set\":      ean_set,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'PARTNUMBER' no están presentes en el archivo de EAN\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"test\", [tallasagotadas_marca_join])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = CONTENEDOR_DESTINO + '/' + homologaciones + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'tallas_agotadas.csv', sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulacion de function App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "import azure.functions as func\n",
    "from centraal_dataframework.blueprints import framework\n",
    "app = func.FunctionApp()\n",
    "app.register_functions(framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es solo para obtener la funciones \n",
    "# dentro del notebook\n",
    "funciones = {fun.get_user_function().__name__ : fun.get_user_function() for fun in app.get_functions()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:se programan ['scrapping_validate_column_contents', 'marcaspropias_validate_column_contents', 'homologaciones_validar_cantidad', 'ordentallas_validar_join', 'ean_validar_cantidad_registros', 'atributos_validar_referencias', 'tallasagotadas_validar_join']\n"
     ]
    }
   ],
   "source": [
    "with patch(\"azure.functions.Out\", autospec=True) as mock_msg:\n",
    "    timer = func.timer.TimerRequest(past_due = False)\n",
    "    funciones['check_and_schedule_task'](timer, mock_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TAREA: scrapping_validate_prerequisites--2023-12-05 12:07:30,826-INFO-Validación exitosa, se promueve a cleansed-zone\n",
      "INFO:scrapping_validate_prerequisites:Validación exitosa, se promueve a cleansed-zone\n",
      "ERROR:centraal_dataframework.runner:se presento error en scrapping_validate_prerequisites\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/german/repos/centraal-dataframework/centraal_dataframework/runner.py\", line 122, in run_task\n",
      "    func_to_execute()\n",
      "  File \"/home/german/repos/centraal-dataframework/centraal_dataframework/tasks.py\", line 46, in wrapper\n",
      "    return func(datalake, gx_toolkit, logger, *args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5591/1268011102.py\", line 73, in scrapping_validate_prerequisites\n",
      "    logger.info(result['url'])\n",
      "  File \"/home/german/repos/centraal-dataframework/.venv/lib/python3.8/site-packages/great_expectations/types/__init__.py\", line 70, in __getitem__\n",
      "    return getattr(self, item)\n",
      "AttributeError: 'CheckpointResult' object has no attribute 'url'\n",
      "WARNING:root:enviando notificacion a ['german@centraal.studio', 'wilber@centraal.studio']\n"
     ]
    },
    {
     "ename": "ErrorEnTarea",
     "evalue": "Se encontro un error en scrapping_validate_prerequisites.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/centraal-dataframework/centraal_dataframework/runner.py:122\u001b[0m, in \u001b[0;36mRunner.run_task\u001b[0;34m(self, task_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     func_to_execute()\n\u001b[1;32m    123\u001b[0m \u001b[39mexcept\u001b[39;00m ErrorTareaCalidadDatos \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/repos/centraal-dataframework/centraal_dataframework/tasks.py:46\u001b[0m, in \u001b[0;36mtask_dq.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m gx_toolkit \u001b[39m=\u001b[39m GreatExpectationsToolKit(context, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m func(datalake, gx_toolkit, logger, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/german/repos/centraal-dataframework/docs/cyrstal.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/german/repos/centraal-dataframework/docs/cyrstal.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mValidación exitosa, se promueve a cleansed-zone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/german/repos/centraal-dataframework/docs/cyrstal.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(result[\u001b[39m'\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/german/repos/centraal-dataframework/docs/cyrstal.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m csv_output_dir \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCONTENEDOR_DESITNO\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mWD\u001b[39m}\u001b[39;00m\u001b[39m/scrapping.csv\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/repos/centraal-dataframework/.venv/lib/python3.8/site-packages/great_expectations/types/__init__.py:70\u001b[0m, in \u001b[0;36mDictDot.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mkeys())[item]\n\u001b[0;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, item)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CheckpointResult' object has no attribute 'url'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mErrorEnTarea\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/german/repos/centraal-dataframework/docs/cyrstal.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/german/repos/centraal-dataframework/docs/cyrstal.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# ahora simular la ejecucion\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/german/repos/centraal-dataframework/docs/cyrstal.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m funciones[\u001b[39m'\u001b[39;49m\u001b[39mexecute_tasks_inqueue\u001b[39;49m\u001b[39m'\u001b[39;49m](func\u001b[39m.\u001b[39;49mQueueMessage(body \u001b[39m=\u001b[39;49m \u001b[39mbytes\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mscrapping_validate_prerequisites\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
      "File \u001b[0;32m~/repos/centraal-dataframework/centraal_dataframework/blueprints.py:74\u001b[0m, in \u001b[0;36mexecute_tasks_inqueue\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m     72\u001b[0m task_name \u001b[39m=\u001b[39m msg\u001b[39m.\u001b[39mget_body()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mexecute_tasks_queue va ejecutar la tarea: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, task_name)\n\u001b[0;32m---> 74\u001b[0m runner\u001b[39m.\u001b[39;49mrun_task(task_name)\n",
      "File \u001b[0;32m~/repos/centraal-dataframework/centraal_dataframework/runner.py:130\u001b[0m, in \u001b[0;36mRunner.run_task\u001b[0;34m(self, task_name)\u001b[0m\n\u001b[1;32m    128\u001b[0m logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mse presento error en \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, task_name, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    129\u001b[0m send_email_error(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogic_app_url, emails, error_tarea, func_to_execute)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mraise\u001b[39;00m ErrorEnTarea(task_name) \u001b[39mfrom\u001b[39;00m \u001b[39merror_tarea\u001b[39;00m\n",
      "\u001b[0;31mErrorEnTarea\u001b[0m: Se encontro un error en scrapping_validate_prerequisites."
     ]
    }
   ],
   "source": [
    "# ahora simular la ejecucion\n",
    "funciones['execute_tasks_inqueue'](func.QueueMessage(body = bytes(\"scrapping_validate_prerequisites\",  \"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
