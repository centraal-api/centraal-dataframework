{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysftp==0.2.9\n",
    "#Imports presentacion\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports generales\n",
    "import pysftp\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar directorio CDF y Configuraciones\n",
    "sys.path.append('../')\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Importar CDF\n",
    "from centraal_dataframework.resources import datalake\n",
    "from great_expectations.core.expectation_configuration import ExpectationConfiguration\n",
    "from centraal_dataframework.tasks import task_dq, task\n",
    "from centraal_dataframework.excepciones import ErrorTareaCalidadDatos\n",
    "from centraal_dataframework.resources import GreatExpectationsToolKit\n",
    "from centraal_dataframework.runner import Runner\n",
    "\n",
    "# Preparación de ambiente\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENEDOR = \"\"\n",
    "CONTENEDOR_DESITNO = os.environ['datalake_cleandir']\n",
    "WD = os.environ['datalake_workdir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traer Archivos SFTP\n",
    "@task\n",
    "def get_scrapping_file(datalake, logger):\n",
    "    # Variables\n",
    "    last_file_date = 0\n",
    "    last_file_name = ''\n",
    "    scrapping_file = None\n",
    "    csv_output_dir = f\"{CONTENEDOR}/{WD}\"\n",
    "    # Abrir conexión\n",
    "    cnopts = pysftp.CnOpts()\n",
    "    cnopts.hostkeys = None\n",
    "    sftp = pysftp.Connection(\n",
    "        host=os.environ['sftp_servidor'],\n",
    "        port=int(os.environ['sftp_port']),\n",
    "        username=os.environ['sftp_usuario'],\n",
    "        password=os.environ['sftp_clave'],\n",
    "        cnopts=cnopts,\n",
    "    )\n",
    "    sftp.cwd(os.environ['sftp_raiz'])\n",
    "    # Buscar el último archivo\n",
    "    archivos = sftp.listdir_attr()\n",
    "    for archivo in archivos:\n",
    "        if archivo.longname[0] != 'd':\n",
    "            if archivo.st_atime > last_file_date:\n",
    "                last_file_date = archivo.st_atime\n",
    "                last_file_name = archivo.filename\n",
    "    # Cargar el archivo al DataFrame\n",
    "    with sftp.open(last_file_name) as sfile:\n",
    "        scrapping_file = pd.read_csv(sfile, sep=',', encoding='latin1')\n",
    "        sftp.close()\n",
    "    # Escribimos el DataFrame en nuestro raw-zone\n",
    "    datalake.write_csv(scrapping_file, f\"{csv_output_dir}/scrapping.csv\", sep='|', index=False, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def scrapping_validate_prerequisites(datalake, gx_toolkit: GreatExpectationsToolKit, logger):\n",
    "    \"\"\"Valida los pre-requisitos básicos del archivo de Scrapping\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['scrapping_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"scrapping.csv\", sep=\"|\", encoding='latin1')\n",
    "    logger.info(\"Validando prerrequisitos del archivo Scrapping...\")\n",
    "\n",
    "    # Nombres de columnas\n",
    "    scrapping_column_names = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_table_columns_to_match_set\",\n",
    "        kwargs={\n",
    "            \"column_set\": [\n",
    "                \"date\",\n",
    "                \"canal\",\n",
    "                \"category\",\n",
    "                \"subcategory\",\n",
    "                \"subcategory2\",\n",
    "                \"subcategory3\",\n",
    "                \"marca\",\n",
    "                \"modelo\",\n",
    "                \"sku\",\n",
    "                \"upc\",\n",
    "                \"item\",\n",
    "                \"item characteristics\",\n",
    "                \"url sku\",\n",
    "                \"image\",\n",
    "                \"price\",\n",
    "                \"sale price\",\n",
    "                \"shipment cost\",\n",
    "                \"sales flag\",\n",
    "                \"store id\",\n",
    "                \"store name\",\n",
    "                \"store address\",\n",
    "                \"stock\",\n",
    "                \"upc wm\",\n",
    "                \"final price\",\n",
    "                \"upc wm2\",\n",
    "                \"comp\",\n",
    "                \"composition\",\n",
    "                \"homogenized_clothing\",\n",
    "                \"homogenized_subcategory\",\n",
    "                \"homogenized_category\",\n",
    "                \"homogenized_color\",\n",
    "                \"made_in\",\n",
    "            ],\n",
    "            \"exact_match\": True,\n",
    "            \"result_format\": \"SUMMARY\",\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Las columnas del archivo de Scrapping no concuerdan con las esperadas\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Price Not Null\n",
    "    scrapping_price_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\"column\": \"price\", \"result_format\": \"SUMMARY\"},\n",
    "        meta={\n",
    "            \"notes\": {\"format\": \"markdown\", \"content\": \"Algunos precios del archivo de Scrapping no están presentes\"}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # EXECUTE EXPECTATIONS\n",
    "    url, result = gx_toolkit.run_expectations_on_df(\n",
    "        source, \"SCRAPPING_MANDATORY\", [scrapping_column_names, scrapping_price_notnull]\n",
    "    )\n",
    "    clear_output(wait=True)\n",
    "    if result.success:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "        logger.info(result['url'])\n",
    "        csv_output_dir = f\"{CONTENEDOR_DESITNO}/{WD}/scrapping.csv\"\n",
    "        datalake.write_csv(source, csv_output_dir, sep=\"|\", encoding='latin1')\n",
    "    else:\n",
    "        logger.info('ERROR: Validación fallida, se detiene el proceso.')\n",
    "        logger.info(result['url'])\n",
    "        raise ErrorTareaCalidadDatos(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def scrapping_validate_column_contents(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Identifica inconsistencias en el contenido de las columnas del archivo Scrapping\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['scrapping_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir+\"scrapping.csv\", sep=\"|\", encoding = 'latin1')\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando contenido de columnas del archivo Scrapping...\")\n",
    "    # creaciones de expectativas\n",
    "\n",
    "    scrapping_clothing_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"clothing\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'clothing' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    #homogenized_category NOT NULL\n",
    "    scrapping_homogenized_category_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"homogenized_category\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_category' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #homogenized_subcategory NOT NULL\n",
    "    scrapping_homogenized_subcategory_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"homogenized_subcategory\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_subcategory' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    #homogenized_color NOT NULL\n",
    "    scrapping_homogenized_color_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"homogenized_color\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_color' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    #marca 9 values (up to 11) <- PARAMETER\n",
    "    scrapping_marca_unique = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_unique_value_count_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"marca\",\n",
    "            \"min_value\":     9,\n",
    "            \"max_value\":     11,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'homogenized_color' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"SCRAPPING_CONSISTENCE\", [scrapping_clothing_notnull, scrapping_homogenized_category_notnull,\n",
    "                                                                              scrapping_homogenized_subcategory_notnull, scrapping_homogenized_color_notnull,\n",
    "                                                                              scrapping_marca_unique])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['scrapping_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'scrapping.csv', sep=\"|\", encoding = 'latin1')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def scrapping_validate_prices(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida el rango de precios del archivo de Scrapping\n",
    "       Según el lote Zara y de las demás marcas\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['scrapping_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"scrapping.csv\", sep=\"|\", encoding = 'latin1')\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando contenido de columnas del archivo Scrapping...\")\n",
    "    # creaciones de expectativas\n",
    "    #Final Price\n",
    "    #Precio Final Between\n",
    "    ##Zara 12.000 - 4.000.000\n",
    "    scrapping_zara_price_range = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"final price\",\n",
    "            \"min_value\":     12000,\n",
    "            \"max_value\":     4000000,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Para la marca Zara, algunos precios no se encuentran en el rango esperado.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ##Otras - 12.000 - 1.300.000\n",
    "    scrapping_otras_price_range = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"final price\",\n",
    "            \"min_value\":     12000,\n",
    "            \"max_value\":     1300000,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos precios no se encuentran en el rango esperado.\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    result_zara  = gx_toolkit.run_expectations_on_df(source[source['canal'] == 'Zara Colombia'], \"SCRAPPING_PRECIOS_ZARA\", [scrapping_zara_price_range])\n",
    "    result_otras = gx_toolkit.run_expectations_on_df(source[source['canal'] != 'Zara Colombia'], \"SCRAPPING_PRECIOS\", [scrapping_otras_price_range])\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    if result_zara['status'] and result_otras['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info('ZARA:  '+result_zara['url'])\n",
    "        logger.info('OTRAS: '+result_otras['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['scrapping_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'scrapping.csv', sep=\"|\", encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def marcaspropias_validate_column_contents(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Validar el contenido de las columnas de marcas propias\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['datasvcs_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"marcas_propias.csv\", sep=\",\", encoding='utf8')\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando contenido de columnas del archivo Marcas Propias...\")\n",
    "    # creaciones de expectativas\n",
    "    marcprop_categoria_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"categoria\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'categoria' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Uso NOT NULL\n",
    "    marcprop_use_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"use\",\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'use' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Tipo Prenda NOT NULL\n",
    "    marcprop_tipo_prenda_notnull = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "        kwargs={\n",
    "            \"column\":        \"prendasGenerales\", #??????\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'prendasGenerales' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"MARCAS_PROPIAS_CONSISTENCY\", [marcprop_categoria_notnull, marcprop_use_notnull,\n",
    "                                                                                   marcprop_tipo_prenda_notnull])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['datasvcs_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'marcas_propias.csv', sep=\"|\", encoding='utf8')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def homologaciones_validar_cantidad(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida la cantidad de marcas que existen en el archivo de homologaciones\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"homologaciones.csv\", sep=\",\")\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando cantidad de registros en homologaciones..\")\n",
    "    homologacion_marca_join = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_unique_value_count_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"Marca\",\n",
    "            \"min_value\":     9,\n",
    "            \"max_value\":     11,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'marca' no están presentes\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"CANTIDAD_MARCAS_HOMOLOGACION\", [homologacion_marca_join])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'homologaciones.csv', sep=\"|\", encoding='utf8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def ordentallas_validar_join(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Validación de Orden Tallas para Join\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"orden_tallas.csv\", sep=\",\")\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['scrapping_workdir'] + '/'\n",
    "    validation_set = datalake.read_csv(csv_input_dir + \"scrapping.csv\", sep=\"|\", encoding='latin1')\n",
    "    tallas_set = validation_set.stock.unique().tolist()\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando JOIN Orden Tallas...\")\n",
    "\n",
    "    ordentallas_marca_join = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_set\",\n",
    "        kwargs={\n",
    "            \"column\":        \"Talla\",\n",
    "            \"value_set\":      tallas_set,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'marca' no están presentes en el archivo de Orden Tallas\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"ORDENTALLA_MARCAS_JOIN\", [ordentallas_marca_join])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'orden_tallas.csv', sep=\"|\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TAREA: ordentallas_validar_join--2023-11-30 08:54:40,978-INFO-WARNING: Validación fallida, continúa.\n",
      "TAREA: ordentallas_validar_join--2023-11-30 08:54:40,978-INFO-Se genera un archivo privado: /calidad-datos\\validations/ordentallas_validar_join_expectation_suite/__none__/20231130T135407.552650Z/ordentallas_validar_join_source-ordentallas_validar_join_source_ORDENTALLA_MARCAS_JOIN.html. Visitar portal azure para acceder resultados.\n"
     ]
    }
   ],
   "source": [
    "ordentallas_validar_join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def ean_validar_cantidad_registros(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida la canitdad de EANs\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"ean.csv\", sep=\"|\")\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando Cantidad EAN...\")\n",
    "    ean_unique = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_unique_value_count_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":        \"EAN\",\n",
    "            \"min_value\":     3000,\n",
    "            \"max_value\":     4000,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"La cantidad de productos presentes excede la esperada\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"CANTIDAD_EAN\", [ean_unique])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'ean.csv', sep=\"|\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TAREA: ean_validar_cantidad_registros--2023-11-30 08:55:19,700-INFO-WARNING: Validación fallida, continúa.\n",
      "TAREA: ean_validar_cantidad_registros--2023-11-30 08:55:19,701-INFO-Se genera un archivo privado: /calidad-datos\\validations/ean_validar_cantidad_registros_expectation_suite/__none__/20231130T135445.252623Z/ean_validar_cantidad_registros_source-ean_validar_cantidad_registros_source_CANTIDAD_EAN.html. Visitar portal azure para acceder resultados.\n"
     ]
    }
   ],
   "source": [
    "ean_validar_cantidad_registros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def atributos_validar_referencias(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Valida la existencia de las referencias para los atributos\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir + \"atributos.csv\", sep=\"|\")\n",
    "    validation_set = datalake.read_csv(csv_input_dir + \"ean.csv\", sep=\"|\")\n",
    "    referencia_set = validation_set.REFERENCIA.unique().tolist()\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando Cantidad EAN...\")\n",
    "    ean_unique = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_set\",\n",
    "        kwargs={\n",
    "            \"column\":        \"REFERENCIA\",\n",
    "            \"value_set\":      referencia_set,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunas referencias del archivo de Atributos no están definidas en Marcas Propias\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"ATRIBUTOS_REFERENCIAS\", [ean_unique])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'atributos.csv', sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TAREA: atributos_validar_referencias--2023-11-30 08:56:03,799-INFO-WARNING: Validación fallida, continúa.\n",
      "TAREA: atributos_validar_referencias--2023-11-30 08:56:03,799-INFO-Se genera un archivo privado: /calidad-datos\\validations/atributos_validar_referencias_expectation_suite/__none__/20231130T135526.261477Z/atributos_validar_referencias_source-atributos_validar_referencias_source_ATRIBUTOS_REFERENCIAS.html. Visitar portal azure para acceder resultados.\n"
     ]
    }
   ],
   "source": [
    "atributos_validar_referencias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_dq\n",
    "def tallasagotadas_validar_join(datalake, gx_toolkit, logger):\n",
    "    \"\"\"Veriifca las referencias de las tallas agotadas\"\"\"\n",
    "    csv_input_dir = os.environ['datalake_workdir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    source = datalake.read_csv(csv_input_dir+'tallas_agotadas.csv', sep='|')\n",
    "    validation_set = datalake.read_csv(csv_input_dir+'ean.csv', sep='|')\n",
    "    ean_set = validation_set.EAN.unique().tolist()\n",
    "    logger.info(source.head(1))\n",
    "    logger.info(\"Validando JOIN Orden Tallas...\")\n",
    "\n",
    "    tallasagotadas_marca_join = ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_in_set\",\n",
    "        kwargs={\n",
    "            \"column\":        \"PARTNUMBER\",\n",
    "            \"value_set\":      ean_set,\n",
    "            \"result_format\": \"SUMMARY\"\n",
    "        },\n",
    "        meta={\n",
    "            \"notes\": {\n",
    "                \"format\": \"markdown\",\n",
    "                \"content\": \"Algunos elementos de la columna 'PARTNUMBER' no están presentes en el archivo de EAN\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    result = gx_toolkit.run_expectations_on_df(source, \"test\", [tallasagotadas_marca_join])\n",
    "    clear_output(wait=True)\n",
    "    if result['status']:\n",
    "        logger.info('Validación exitosa, se promueve a cleansed-zone')\n",
    "    else:\n",
    "        logger.info('WARNING: Validación fallida, continúa.')\n",
    "        logger.info(result['url'])\n",
    "    csv_output_dir = os.environ['datalake_cleandir'] + '/' + os.environ['homologa_workdir'] + '/'\n",
    "    datalake.write_csv(source, csv_output_dir+'tallas_agotadas.csv', sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TAREA: tallasagotadas_validar_join--2023-11-30 08:56:56,808-INFO-WARNING: Validación fallida, continúa.\n",
      "TAREA: tallasagotadas_validar_join--2023-11-30 08:56:56,809-INFO-Se genera un archivo privado: /calidad-datos\\validations/tallasagotadas_validar_join_expectation_suite/__none__/20231130T135609.111781Z/tallasagotadas_validar_join_source-tallasagotadas_validar_join_source_test.html. Visitar portal azure para acceder resultados.\n"
     ]
    }
   ],
   "source": [
    "tallasagotadas_validar_join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
